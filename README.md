# twaddle
Data visualization of website political context - Python coding

2022-07-26 While contemplating my capstone project for the Google Data Analytics certificate I came upon the idea of using data visualization to help users of the WWW understand the political context of articles published on news websites. Reading all the articles, even on one web site would be a large, possibly overwhelming task. I felt that data analytics and visualization could provide an easy, fast and timely snapshot of topics covered on a particular website, and even the ability to contrast several different sites. My first, and working choice for a visualization is the word cloud, which would display and relatively emphasize the frequency of words used on that site. I plan to create a dashboard which will display these visualizations and dynamically update as the sites publish new articles.

Here are some use cases for my dashboard. A relatively uninformed person wants to find what topics are hot news. They go to the twaddle dashboard and view the most frequently used terms on, for example, CNN.com, FoxNews.com, TheHill.com. If a displayed term piques their interest, they can search on that site and read the articles that cover it. A more politically savvy user could contrast the word frequencies among the different sites to get insight into their relative political contexts, and again could then explore topics of interest by searching the site(s).

Because I have experience in Python programming I determined to write some software to collect the word frequency data by treating a web site as a tree structure where each page is a node containing article text and links. The program collects the text data for the page and then recursively follows the links to some arbitrary depth until the whole (depth-limited) tree is explored. Data is collected by downloading the page source code and analyzing to find the relevant code for the article text and links. This is somewhat akin to finding a needle in a haystack, where a web page may have thousands of lines of code and yield maybe a few hundred words and a handful of relevant links. All the words from article texts are saved and tallied, and form the data set for a visualization.

Because of the sheer volume of web pages and types of links on each site I decided to make some working rules about the links I would follow and the articles I would collect. For example, any articles must have a publication date and be no older than some arbitrary time span. My initial setting for this is 48 hours. For links, only follow ones in the website's domain, e.g., not going off to twitter or other sites, such as advertisements. Also, from experience in viewing and collecting the site, I implement some heuristic (read arbitrary) rules to disregard some subsections of the sites, e.g., collections of articles listed by author and not necessarily timely, or television program reviews. These kind of decisions require some "getting my feet wet" by wading through what's collected, and will likely require specific code for each web site analyzed.

At this time I have only explored and tuned my parser for TheHill.com. I chose that site because it seems to focus on political news and seems to be politically neutral. I am currently able to collect the data for TheHill.com and I am beginning the software to clean the data which will require taking into account differences in capitalization, removing stray formatting characters or other non-text data, and determining a set of "throw-away" words which, while used commonly, are not relevant for contextual understanding. For example, the most frequently collected word is generally "the". I'll try to explain any heuristic I use to generate that set. I plan to post my code and log my work here. For the dashboard I am planning to try Tableau.

My working name for the project is Twaddle, which means "trivial or foolish speech or writing; nonsense", so I'm poking fun at myself, but my aim is to provide a pointer to currently relevant topics in a way that is interesting and fun.

2022-08-19 This will probably be my last entry for this project, I'll consider it a wrap-up description.  Unfortunately, I've abandoned my goal of creating an automatically-updating dashboard, finding it beyond the easy implementation of my web parsers.  My assumptions that most web pages could easily be read turned out to be overly naive.  I had planned to create a display based on 3 web sites with assumed different political perspectives.  I planned to use FoxNews.com for the conservative viewpoint, CNN.com or MSNBC.com for the liberal, and TheHill.com for a neutral point.  

My first target wass thehill.com, and by visually inspecting the html code I was able to make a reliable parser to collect the article words and links for twaddle.  

Next I tried to use that parser for FoxNews.com and it failed miserably.  Again I inspected the html code and determined a different scheme for the text of articles on that site.  So I had to build a new parser which ultimately worked pretty well and allowed me to collect the data I needed.  

Next I tried CNN, and based on my experience with Fox, I decided to forego trying the parser, and just begin by inspecting the page code, expecting to make a new parser, tuned to CNN.  I was shocked by the complexity of that code and I found it so inscrutable that it was impractical to try and figure it out.  I could see the article text, but it was all mixed in with what looked like a total mish-mash, which could only be deciphered by a modern web browser, not my simple parser.  

So, I hoped MSNBC would be a better candidate.  WRONG!  The page code for MSNBC's articles didn't even contain the text.  It was somewhere else, known only to a browser's brains and the software used by MSNBC to create it.  Since this is a practice project I don't want to get deeply into the creation of modern web pages.  Back in the old days I used to hand code web sites!  So I decided to choose another polically-oriented web site to include in the results, the Institute for Cultural Evolution, www.culturalevolution.org, an organization dedicated to transcending and including world views from across the political spectrum, and promoting solutions based on that evolved perspective.  Luckily, I found their page code to be more traditional, similary to thehill.com.  I was able to use my original parser to collect that data.

