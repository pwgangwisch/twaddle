# twaddle
Data visualization of website political context - Python coding
2022-07-26 While contemplating my capstone project for the Google Data Analytics certificate I came upon the idea of using data visualization to help users of the WWW understand the political context of articles published on news websites. Reading all the articles, even on one web site would be a large, possibly overwhelming task. I felt that data analytics and visualization could provide an easy, fast and timely snapshot of topics covered on a particular website, and even the ability to contrast several different sites. My first, and working choice for a visualization is the word cloud, which would display and relatively emphasize the frequency of words used on that site. I plan to create a dashboard which will display these visualizations and dynamically update as the sites publish new articles.

Here are some use cases for my dashboard. A relatively uninformed person wants to find what topics are hot news. They go to the twaddle dashboard and view the most frequently used terms on, for example, CNN.com, FoxNews.com, TheHill.com. If a displayed term piques their interest, they can search on that site and read the articles that cover it. A more politically savvy user could contrast the word frequencies among the different sites to get insight into their relative political contexts, and again could then explore topics of interest by searching the site(s).

Because I have experience in Python programming I determined to write some software to collect the word frequency data by treating a web site as a tree structure where each page is a node containing article text and links. The program collects the text data for the page and then recursively follows the links to some arbitrary depth until the whole (depth-limited) tree is explored. Data is collected by downloading the page source code and analyzing to find the relevant code for the article text and links. This is somewhat akin to finding a needle in a haystack, where a web page may have thousands of lines of code and yield maybe a few hundred words and a handful of relevant links. All the words from article texts are saved and tallied, and form the data set for a visualization.

Because of the sheer volume of web pages and types of links on each site I decided to make some working rules about the links I would follow and the articles I would collect. For example, any articles must have a publication date and be no older than some arbitrary time span. My initial setting for this is 48 hours. For links, only follow ones in the website's domain, e.g., not going off to twitter or other sites, such as advertisements. Also, from experience in viewing and collecting the site, I implement some heuristic (read arbitrary) rules to disregard some subsections of the sites, e.g., collections of articles listed by author and not necessarily timely, or television program reviews. These kind of decisions require some "getting my feet wet" by wading through what's collected, and will likely require specific code for each web site analyzed.

At this time I have only explored and tuned my parser for TheHill.com. I chose that site because it seems to focus on political news and seems to be politically neutral. I am currently able to collect the data for TheHill.com and I am beginning the software to clean the data which will require taking into account differences in capitalization, removing stray formatting characters or other non-text data, and determining a set of "throw-away" words which, while used commonly, are not relevant for contextual understanding. For example, the most frequently collected word is generally "the". I'll try to explain any heuristic I use to generate that set. I plan to post my code and log my work here. For the dashboard I am planning to try Tableau.

My working name for the project is Twaddle, which means "trivial or foolish speech or writing; nonsense", so I'm poking fun at myself, but my aim is to provide a pointer to currently relevant topics in a way that is interesting and fun.
